# -*- coding: utf-8 -*-
"""Classification of Sloth

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JH1kjQXPqroNN3Osu2V1W2I-vcrCP_wr
"""

!pip install lazypredict
!pip install lightgbm

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
import warnings
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import math
from lazypredict.Supervised import LazyClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix
# Filter out specific warning by category
warnings.filterwarnings("ignore")
# Your code here that triggers the warning
drive.mount('/content/drive')

"""# Data Understanding

## Data loading
"""

train_df = pd.read_csv("/content/drive/MyDrive/portofolio/Classification of sloth/dataset.csv")
train_df.drop(["Unnamed: 0"], axis=1, inplace=True)
train_df.head()

"""## Exploratory Data Analysis

### Apa saja jenis variabel pada dataset?
"""

train_df.info()

"""### Apakah ada data type yang tidak sesuai dengan value?

hasil dari df.info() menunjukan TYPE, STATE, ADMINISTRATIVE_AREA_LEVEL_2, LOCALITY, SUBLOCALITY merupakan data nominal dan BATH merupakan data diskrit.      
"""

train_df['endangered'].unique()

train_df['specie'].unique()

train_df['sub_specie'].unique()

numeric_cols = train_df.select_dtypes(include=['int', 'float']).columns.tolist()
categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
print(numeric_cols)
print(categorical_cols)

train_df[categorical_cols] = train_df[categorical_cols].astype('category')

categorical_cols = train_df.select_dtypes(include=['category']).columns.tolist()

train_df.info()

"""### Apakah ada missing value?"""

train_df.isna().sum()

"""### Apakah ada outlier value?"""

count_numeric_cols = len(numeric_cols)
num_rows = (count_numeric_cols + 1) // 2  # Calculate the number of rows needed for the grid layout

fig, axes = plt.subplots(num_rows, 2, figsize=(15, 5*num_rows))  # Create a grid layout
axes = axes.flatten()  # Flatten the 2D array of axes to simplify indexing

for index, col in enumerate(numeric_cols):
    sns.boxplot(x=train_df[col], ax=axes[index])
    axes[index].set_title(col)

# Hide any remaining empty subplots
for ax in axes[count_numeric_cols:]:
    ax.axis('off')

plt.tight_layout()
plt.show()

fig, axes = plt.subplots(num_rows, 2, figsize=(15, 5*num_rows))  # Create a grid layout
axes = axes.flatten()  # Flatten the 2D array of axes to simplify indexing
for index, col in enumerate(numeric_cols):
  Q1 = train_df[col].quantile(0.25)
  Q3 = train_df[col].quantile(0.75)
  IQR=Q3-Q1
  train_df[col]=train_df[col][~((train_df[col]<(Q1-2.5*IQR))|(train_df[col]>(Q3+2.5*IQR)))]
  # Impute missing values with the median
  sns.boxplot(x=train_df[col], ax=axes[index])
  axes[index].set_title(col)
train_df.dropna(axis=1, inplace=True)
# Hide any remaining empty subplots
for ax in axes[count_numeric_cols:]:
    ax.axis('off')
plt.tight_layout()
plt.show()

"""### Bagaimana distribusi variabel dalam dataset?"""

train_df.describe(include="all")

"""**Univariate Analysis**

**Categorical Features**
"""

count_categorical_cols = len(categorical_cols)
num_rows = (count_categorical_cols + 1) // 2  # Calculate the number of rows needed for the grid layout

fig, axes = plt.subplots(num_rows, 2, figsize=(15, 5*num_rows))  # Create a grid layout
axes = axes.flatten()  # Flatten the 2D array of axes to simplify indexing

for index, col in enumerate(categorical_cols):
    count = train_df[col].value_counts()
    percent = 100 * train_df[col].value_counts(normalize=True).round(2)
    df = pd.DataFrame({'jumlah sampel': count, 'persentase': percent})
    ax = axes[index]
    count.plot(kind='bar', ax=ax, title=col)
    ax.set_ylabel('Count')

# Hide any remaining empty subplots
for ax in axes[count_categorical_cols:]:
    ax.axis('off')

plt.tight_layout()
plt.show()

"""**Numerical Features**"""

train_df.hist(bins=50, figsize=(20,15))
plt.show()

"""**Multivariate Analysis**

**Categorical Feature**
"""

for col in categorical_cols:
  if col == "specia":
    continue
  sns.catplot(x=col, y="specie", kind="bar", dodge=False, height = 4, aspect = 3,  data=train_df, palette="Set3")
  plt.title("Rata-rata 'specie' Relatif terhadap - {}".format(col))
plt.show()

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(train_df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = train_df.corr(method='spearman').round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""## Data Preparation

### Encoding Fitur Kategori
"""

train_df.drop(["endangered", "sub_specie"], axis=1, inplace=True)

categorical_cols = train_df.select_dtypes(include=['category']).columns.tolist()
categorical_column = []
for i in categorical_cols:
  if i == "specie":
    continue
  train_df = pd.concat([train_df, pd.get_dummies(train_df[i], prefix=i)],axis=1)
  categorical_column.append(i)
train_df.drop(categorical_column, axis=1, inplace=True)
train_df.head()

enc = LabelEncoder()
train_df["specie"] = enc.fit_transform(train_df[["specie"]])

enc.classes_

"""### PCA"""

train_df.head()

pca = PCA(n_components=3, random_state=123)
pca.fit(train_df[["claw_length_cm", "tail_length_cm", "weight_kg"]])
princ_comp = pca.transform(train_df[["claw_length_cm", "tail_length_cm", "weight_kg"]])

pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=123)
pca.fit(train_df[["claw_length_cm", "tail_length_cm", "weight_kg"]])
train_df['body'] = pca.transform(train_df.loc[:, ("claw_length_cm", "tail_length_cm", "weight_kg")]).flatten()
train_df.drop(["claw_length_cm", "tail_length_cm", "weight_kg"], axis=1, inplace=True)

"""### Standarisasi"""

X = train_df.drop(["specie"],axis=1)
y = train_df["specie"]

scaler = StandardScaler()
numeric_cols = ["body"]
scaler.fit(X[numeric_cols])
X[numeric_cols] = scaler.transform(X.loc[:, numeric_cols])
X[numeric_cols].head()

"""### Train-Test-Split"""

X.head()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 30, random_state = 123)

"""## Model Development

### Model Development dengan lazy regressor
"""

clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)
models,predictions = clf.fit(X_train, X_test, y_train, y_test)
# Get the best model based on Accuracy
best_model = models.loc[models['Accuracy'].idxmax()]
print("Best model based on Accuracy:")
print(best_model)

models

param_grid = {
    'C': [0.1, 1, 10, 100],
    'max_iter': [1000, 2000, 3000],
}

grid_search = GridSearchCV(LinearSVC(), param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)
print("Best hyperparameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

"""## Evaluation Model"""

precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

evaluation = {
    "accuracy":[accuracy],
    "precision":[precision],
    "recall":[recall],
    "f1 score":[f1]
}
evaluation_df = pd.DataFrame(evaluation)
evaluation_df

conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", cbar=False)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()